from fastapi import FastAPI
import json
import os
import requests
import redis
from datetime import datetime
from contextlib import asynccontextmanager
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from crawler.naver_crawler import crawl_category, CATEGORIES
from AI.keyword_extractor import extract_keywords

# ==================================================
# Spring API
# ==================================================
SPRING_HOST = os.getenv("SPRING_HOST", "localhost")
REDIS_HOST = os.getenv("REDIS_HOST", "127.0.0.1")

SPRING_NEWS_API = f"http://{SPRING_HOST}:8080/api/internal/news"

rd = redis.Redis(
    host=REDIS_HOST,
    port=6379,
    db=0,
    decode_responses=True
)
# ==================================================
# Paths
# ==================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "Data", "raw", "naver")

# ==================================================
# Utils
# ==================================================
def normalize_time(raw_time):
    if not raw_time or raw_time in ["ì‹œê°„ ë¯¸ìƒ", "", "null"]:
        return None
    try:
        return datetime.fromisoformat(raw_time).isoformat()
    except Exception:
        return None


def normalize_keywords(keywords):
    cleaned = []
    for item in keywords:
        if not isinstance(item, (list, tuple)) or len(item) != 2:
            continue
        word, score = item
        try:
            cleaned.append((str(word), float(score)))
        except Exception:
            continue
    return cleaned


def send_news_to_spring(article, category):
    # [ìˆ˜ì •] DBì—ë„ í‚¤ì›Œë“œê°€ ì €ìž¥ë˜ì–´ì•¼ ë‚˜ì¤‘ì— ìºì‹±í•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŒ
    payload = {
        "category": category,
        "title": article.get("title"),
        "content": article.get("content"),
        "url": article.get("url"),
        "press": article.get("press"),
        "published_at": normalize_time(article.get("published_at")),
        "keywords": article.get("keywords")  # ðŸ”¥ [ì¤‘ìš”] í‚¤ì›Œë“œ í•„ë“œ ì¶”ê°€
    }

    try:
        res = requests.post(SPRING_NEWS_API, json=payload, timeout=3)
        if res.status_code != 200:
            print("[SPRING ERROR]", res.status_code, res.text)
    except Exception as e:
        print("[SPRING CONNECT ERROR]", e)


# ==================================================
# Crawler Job
# ==================================================
def scheduled_crawl_all_categories():
    print("\nðŸ”¥ [CRAWLER] START")

    for category, code in CATEGORIES.items():
        category = category.lower()

        try:
            print(f"\n[CRAWLER] crawling {category}")

            # 1ï¸âƒ£ í¬ë¡¤ë§
            crawl_category(category, code, max_pages=1)

            file_path = os.path.join(DATA_DIR, f"{category}.json")
            if not os.path.exists(file_path):
                print("[SKIP] no file", file_path)
                continue

            with open(file_path, "r", encoding="utf-8") as f:
                articles = json.load(f)

            if not articles:
                print("[SKIP] no articles")
                continue

            # 2ï¸âƒ£ Springìœ¼ë¡œ ë‰´ìŠ¤ ì €ìž¥ (MySQL ì €ìž¥)
            for article in articles:
                send_news_to_spring(article, category)

            # 3ï¸âƒ£ [ì‚­ì œë¨] Redis ê¸°ì‚¬ ìºì‹œ (Cache-Aside íŒ¨í„´ ì ìš©)
            # Pythonì€ ì´ì œ Redisì— ê¸°ì‚¬ ë³¸ë¬¸ì„ ì €ìž¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            # Java(Spring)ê°€ ì¡°íšŒ ì‹œì ì— DBì—ì„œ ê°€ì ¸ì™€ Redisì— ë„£ìŠµë‹ˆë‹¤.
            
            # ê¸°ì¡´ ê¸°ì‚¬ ìºì‹œ ì‚­ì œ (í˜¹ì‹œ ë‚¨ì•„ìžˆì„ ë°ì´í„° ì •ë¦¬ìš©)
            article_key = f"trend:{category}:articles"
            rd.delete(article_key) 
            
            print(f"[CACHE-ASIDE] Skipped Redis write for {category}. Data sent to DB.")


            # 4ï¸âƒ£ í‚¤ì›Œë“œ ì¶”ì¶œ & ì ìˆ˜ ì €ìž¥ (ëž­í‚¹ ì‹œìŠ¤í…œì€ ìœ ì§€)
            # ê¸°ì‚¬ ë³¸ë¬¸ ìºì‹œì™€ ë‹¬ë¦¬, 'ì‹¤ì‹œê°„ í‚¤ì›Œë“œ ëž­í‚¹'ì€ Redisê°€ ì›ë³¸ì´ë¯€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
            score_key = f"trend:{category}:scores"
            rd.delete(score_key)

            raw_keywords = extract_keywords(category)
            keywords = normalize_keywords(raw_keywords)

            print(f"[KEYWORDS] {category} -> {keywords}")

            # ì ìˆ˜ ëˆ„ì 
            for word, score in keywords:
                rd.zincrby(score_key, int(score * 100), word)

            # ðŸ”¥ ìƒìœ„ 10ê°œë§Œ ë‚¨ê¸°ê¸°
            top_keywords = rd.zrevrange(score_key, 0, 9, withscores=True)

            rd.delete(score_key)
            for word, score in top_keywords:
                rd.zadd(score_key, {word: score})

            rd.expire(score_key, 86400)

            print(
                f"[REDIS SAVED] {category} scores ->",
                rd.zrange(score_key, 0, -1, withscores=True)
            )

        except Exception as e:
            print(f"[CRAWLER ERROR] {category}", e)

    print("\nðŸ”¥ [CRAWLER] END\n")


# ==================================================
# FastAPI Lifespan
# ==================================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler = AsyncIOScheduler()

    # ðŸ”¥ ì„œë²„ ì‹œìž‘ ì‹œ 1íšŒ ì¦‰ì‹œ ì‹¤í–‰
    scheduled_crawl_all_categories()

    scheduler.add_job(
        scheduled_crawl_all_categories,
        trigger="interval",
        minutes=10,
        id="news_crawler"
    )

    scheduler.start()
    print("[SCHEDULER] started")

    yield

    scheduler.shutdown()
    print("[SCHEDULER] stopped")


# ==================================================
# App
# ==================================================
app = FastAPI(lifespan=lifespan)


if __name__ == "__main__":
    import uvicorn

    print("REDIS PING:", rd.ping())
    uvicorn.run(app, host="0.0.0.0", port=8000)